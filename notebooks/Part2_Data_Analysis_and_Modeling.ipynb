{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89e8585-1c26-495d-b132-7da4da94a1be",
   "metadata": {},
   "source": [
    "# Data Analysis And Perdiction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7782d84",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c49a2-5f8b-4dac-a110-b08b01ffef36",
   "metadata": {},
   "source": [
    "### In this section, we import the necessary libraries and packages for our analysis. `pandas` is used for data manipulation, `numpy` for numerical operations, and `matplotlib.pyplot` for creating visualizations. We use `re` (regular expressions) for text and data parsing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3499f7-4e81-4876-8803-dd704961eee3",
   "metadata": {},
   "source": [
    "### For machine learning tasks, we utilize several components from `scikit-learn`: `ElasticNet` for regression models, `train_test_split` and `GridSearchCV` for model validation, `StandardScaler` and `OneHotEncoder` for data preprocessing, `Pipeline` to streamline workflows, and `ColumnTransformer` for applying transformers to columns of an array or pandas DataFrame. Lastly, `mean_squared_error` is used to evaluate the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7cea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\desktop\\code_projects\\data mining and analylisis\\dataanalysis_venv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\lenovo\\desktop\\code_projects\\data mining and analylisis\\dataanalysis_venv\\lib\\site-packages (from scikit-learn) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\lenovo\\desktop\\code_projects\\data mining and analylisis\\dataanalysis_venv\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\desktop\\code_projects\\data mining and analylisis\\dataanalysis_venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\desktop\\code_projects\\data mining and analylisis\\dataanalysis_venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade scikit-learn\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918db06",
   "metadata": {},
   "source": [
    "# File reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4b5da-885e-495a-bd0d-50ab9d1312f6",
   "metadata": {},
   "source": [
    "### Here, we read the data collected from the web scraping process into a pandas DataFrame to prepare it for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c78133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input(\"Please enter the path of your CSV file here (Write the path without quotation marks: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63de0e",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3e151-4727-41f1-b14c-02fbed50a97a",
   "metadata": {},
   "source": [
    "### This section includes functions and code to clean and preprocess the data. This  involves handling missing values, correcting data types, and creating new features from the existing data to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47485b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \n",
    "    # After reviewing the 'manufactor' column, we found duplicates\n",
    "    df['manufactor'] = df['manufactor'].replace('Lexsus', 'לקסוס')\n",
    "\n",
    "    # After revewing the model column, we saw that some of the model names are written with the name of the manufactor and\n",
    "    # the year saw we build a function to help us clean the column:\n",
    "    def clean_model_name(manufactor, model_name):\n",
    "        model_name = re.sub(r'\\(\\d{4}\\)', '', model_name)\n",
    "        model_name = re.sub(manufactor, '', model_name, flags=re.IGNORECASE)\n",
    "        model_name = model_name.strip()\n",
    "        return model_name\n",
    "\n",
    "    df['model'] = df.apply(lambda row: clean_model_name(row['manufactor'], row['model']), axis=1)\n",
    "    \n",
    "    # Further cleaning 'model', 'Gear', and 'Engine_type' columns for remaining duplicates\n",
    "    df['model'] = df['model'].replace('CIVIC', 'סיוויק')\n",
    "    df['model'] = df['model'].replace('ACCORD', 'אקורד')\n",
    "    df['model'] = df['model'].replace(\"ג`טה\", \"ג'טה\")\n",
    "    df['model'] = df['model'].replace(\"ג'אז\", \"ג`אז\")\n",
    "    df['model'] = df['model'].replace('מיטו / MITO', 'מיטו')\n",
    "    df['model'] = df['model'].replace(\"ג'וק JUKE\", \"ג'וק\")\n",
    "    df['Gear'] = df['Gear'].replace('לא מוגדר', np.nan)\n",
    "    df['Gear'] = df['Gear'].replace('אוטומט', 'אוטומטית')\n",
    "    df['Engine_type'] = df['Engine_type'].replace('היבריד', 'היברידי')\n",
    "    \n",
    "    # Converting 'capacity_Engine' to numeric and removing outliers, assuming minimum capacity is 800\n",
    "    df['capacity_Engine'] = pd.to_numeric(df['capacity_Engine'], errors='coerce')\n",
    "    df.loc[df['capacity_Engine'] < 800, 'capacity_Engine'] = np.nan\n",
    "    \n",
    "    # We assume that cars with the same manufactor and model will have the same capacity engine, gear and engine type\n",
    "    def fill_missing_values(row, reference_data, columns_to_fill, reference_columns):\n",
    "        for col in columns_to_fill:\n",
    "            if pd.isnull(row[col]):\n",
    "                # find a not missing values from similar cars\n",
    "                similar_rows = reference_data[\n",
    "                    (reference_data[reference_columns[0]] == row[reference_columns[0]]) &\n",
    "                    (reference_data[reference_columns[1]] == row[reference_columns[1]])\n",
    "                ]\n",
    "                if not similar_rows.empty:\n",
    "                    non_na_values = similar_rows[col].dropna()\n",
    "                    if not non_na_values.empty:\n",
    "                        row[col] = non_na_values.mode().values[0]\n",
    "        return row\n",
    "    \n",
    "    columns_to_fill = ['Gear', 'Engine_type', 'capacity_Engine']\n",
    "    reference_columns = ['manufactor', 'model']\n",
    "    df = df.apply(lambda row: fill_missing_values(row, df, columns_to_fill, reference_columns), axis=1)\n",
    "    \n",
    "    # If there are still missing values, fill them with the most common value\n",
    "    df['Engine_type'] = df['Engine_type'].replace(np.nan, 'דיזל')\n",
    "    df['Gear'] = df['Gear'].replace(np.nan, 'אוטומטית')\n",
    "    \n",
    "    # Correcting engine capacity for Mitsubishi models based on additional research\n",
    "    df.loc[(df['model'] == 'לנסר') | (df['model'] == 'לנסר הדור החדש'), 'capacity_Engine'] = 1500\n",
    "    df.loc[df['model'] == \"אטראז'\", 'capacity_Engine'] = 1200\n",
    "    df.loc[df['model'] == 'גרנדיס', 'capacity_Engine'] = 2400\n",
    "    df.loc[df['model'] == 'IS300H', 'capacity_Engine'] = 2500\n",
    "    df.loc[df['model'] == 'GT3000', 'capacity_Engine'] = 1600\n",
    "    \n",
    "    # Removing rows with missing 'capacity_Engine' because most of the data is non-null\n",
    "    df = df.dropna(subset=['capacity_Engine'])\n",
    "    \n",
    "    # Assuming the previous owner of a first-hand car is the manufacturer\n",
    "    df.loc[df['Hand'] == 1, 'Prev_ownership'] = 'יצרן'\n",
    "    \n",
    "    # Merging categories in 'Prev_ownership' with small value counts\n",
    "    categories_to_merge = [\"ליסינג\", \"השכרה\", \"חברה\", \"ממשלתי\", \"אחר\", \"מונית\"]\n",
    "    df['Prev_ownership'] = df['Prev_ownership'].replace('לא מוגדר', np.nan)\n",
    "    df['Prev_ownership'] = df['Prev_ownership'].replace(categories_to_merge, 'לא פרטית')\n",
    "    \n",
    "    # We assume that the previous owner of a car which is first hand is the manufactor\n",
    "    df.loc[df['Hand'] == 1, 'Prev_ownership'] = 'יצרן'\n",
    "    \n",
    "    # Function to fill missing values in the \"Prev_ownership column\" by the distribution\n",
    "    def fill_na_proportionally(series):\n",
    "        # finding the distribution\n",
    "        distribution = series[series != 'יצרן'].value_counts(normalize=True)\n",
    "        # Creating a copy\n",
    "        series_copy = series.copy()\n",
    "        # finding missing indices\n",
    "        na_indices = series_copy[series_copy.isna()].index\n",
    "        # creating random values by the distribution\n",
    "        fill_values = np.random.choice(distribution.index, size=len(na_indices), p=distribution.values)\n",
    "        # filling missing values\n",
    "        series_copy.loc[na_indices] = fill_values\n",
    "        return series_copy\n",
    "\n",
    "    df['Prev_ownership'] = fill_na_proportionally(df['Prev_ownership'])\n",
    "    \n",
    "    # Dropping 'Curr_ownership' column because we assume that the current ownership is private\n",
    "    df = df.drop(columns=['Curr_ownership'])\n",
    "    \n",
    "    # Dropping 'City' and 'Area' columns due to messiness and assumed irrelevance to price\n",
    "    df = df.drop(columns=['City', 'Area'])\n",
    "    \n",
    "    # Converting 'Km' column from string to numeric format\n",
    "    df['Km'] = df['Km'].str.replace(',', '')\n",
    "    df['Km'] = df['Km'].replace({'0': np.nan, 'None': np.nan})\n",
    "    df['Km'] = pd.to_numeric(df['Km'], errors='coerce')\n",
    "    \n",
    "    # Correcting mileage written in thousands (140 instead of 140,000)\n",
    "    df.loc[df['Km'] < 1000, 'Km'] = df['Km'] * 1000\n",
    "    np.set_printoptions(suppress=True)\n",
    "   \n",
    "    # Filling missing 'Km' values based on average annual mileage data\n",
    "    def fill_km(row):\n",
    "        if pd.isna(row['Km']):\n",
    "            age = 2024 - row['Year']\n",
    "            if row['Prev_ownership'] in ['פרטית', 'יצרן']:\n",
    "                return age * 15300\n",
    "            elif row['Prev_ownership'] in ['לא פרטית']:\n",
    "                return age * 27400\n",
    "        return row['Km']\n",
    "    \n",
    "    df['Km'] = df.apply(fill_km, axis=1)\n",
    "    \n",
    "    # Feature Engineering:\n",
    "    \n",
    "    # Authorized Service - A binary column indicating whether the car is serviced at an authorized garage based on the description\n",
    "    df['Authorized_service'] = np.where(df['Description'].str.contains('מוסך מורשה', na=False), 1, 0)\n",
    "    \n",
    "    # Km Per Year, removing outliers\n",
    "    df[\"KM_Per_Year\"] = df[\"Km\"] / (2024 - df[\"Year\"])\n",
    "    pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "    df = df[df['KM_Per_Year'] <= 100000]\n",
    "\n",
    "    # Engine efficiency metric based on engine capacity and annual mileage\n",
    "    df['capacity_Engine'] = df['capacity_Engine'].astype(str)\n",
    "    df['capacity_Engine'] = df['capacity_Engine'].str.replace(',', '').astype(float)\n",
    "    df['Engine_Efficiency'] = df['capacity_Engine'] / df['KM_Per_Year']\n",
    "    \n",
    "    # Vintage Car - A car that is at least 30 years old\n",
    "    df['Vintage_Car'] = np.where(df['Year'] < 1995, 1, 0)\n",
    "\n",
    "    # Dropping columns deemed irrelevant to the data\n",
    "    df = df.drop(columns=['Cre_date', 'Repub_date', 'Pic_num', 'Description'])\n",
    "    \n",
    "    # Dropping columns with too many missing values\n",
    "    df = df.drop(columns=['Color', 'Test', 'Supply_score'])\n",
    "    \n",
    "    # Preparing categorical columns for OneHotEncoder\n",
    "    categorical_columns = ['manufactor', 'model', 'Gear', 'Engine_type', 'Prev_ownership']\n",
    "\n",
    "    # Creating ColumnTransformer with OneHotEncoder for categorical columns\n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Fitting the encoder and transforming categorical columns\n",
    "    df_encoded = column_transformer.fit_transform(df)\n",
    "\n",
    "    # Getting names of the encoded columns\n",
    "    encoded_columns = column_transformer.named_transformers_['cat'].get_feature_names_out(categorical_columns)\n",
    "\n",
    "    # Creating a new DataFrame with encoded columns and remaining original columns\n",
    "    df_encoded = pd.DataFrame(df_encoded, columns=encoded_columns.tolist() + df.drop(columns=categorical_columns).columns.tolist())\n",
    "\n",
    "    # Converting encoded columns to numeric type\n",
    "    for col in encoded_columns:\n",
    "        df_encoded[col] = pd.to_numeric(df_encoded[col])\n",
    "\n",
    "    df = df_encoded\n",
    "    # Changing all columns to their correct type\n",
    "    # Adjust the types according to the columns available after transformation\n",
    "    columns_to_convert = {\n",
    "        'Year': 'int',  \n",
    "        'Hand': 'int', \n",
    "        'capacity_Engine': 'float', \n",
    "        'Price': 'float', \n",
    "        'Km': 'float',        \n",
    "        \"Authorized_service\": 'int',\n",
    "        \"KM_Per_Year\": 'float',               \n",
    "        \"Engine_Efficiency\": 'float',\n",
    "        \"Vintage_Car\": 'int'\n",
    "    }\n",
    "\n",
    "    for col, col_type in columns_to_convert.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(col_type)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5982aa7",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5985f-6aa6-4f2b-b392-6ed33edf9b89",
   "metadata": {},
   "source": [
    "### In the forecasting section, we separate the features from the target variable, which is the price. We use various machine learning techniques to predict car prices based on the features we've prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20829d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forcast(df):\n",
    "    # Separating the target variable from the features\n",
    "    y = df['Price']\n",
    "    X = df.drop(columns=['Price'])\n",
    "    \n",
    "    # Creating a pipeline for scaling and model training\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Scaling all features\n",
    "        ('model', ElasticNet())\n",
    "    ])\n",
    "    \n",
    "    # Defining the parameter grid for the search\n",
    "    param_grid = {\n",
    "        'model__alpha': np.logspace(-4, 1, 10),\n",
    "        'model__l1_ratio': np.linspace(0, 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Performing Grid Search\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Getting the best results\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    # Evaluating the model on the entire data\n",
    "    y_pred = best_model.predict(X)\n",
    "    \n",
    "    # Limiting the prediction value\n",
    "    y_pred = np.where(y_pred < 10000, 15000, y_pred)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    \n",
    "    # Getting the coefficients of the model after scaling\n",
    "    model = best_model.named_steps['model']\n",
    "    scaler = best_model.named_steps['scaler']\n",
    "    coefficients = model.coef_\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Creating a DataFrame with the coefficients and their importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': np.abs(coefficients),\n",
    "        'Coefficient': coefficients\n",
    "    })\n",
    "    feature_importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "    \n",
    "    # Top five features\n",
    "    top_5_features = feature_importance.head(5)\n",
    "    top_5_features['Effect'] = top_5_features['Coefficient'].apply(lambda x: 'Positive' if x > 0 else 'Negative')\n",
    "    \n",
    "    print(\"Top 5 features:\")\n",
    "    print(top_5_features[['Feature', 'Importance', 'Effect']])\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdaebc0-181c-44d5-aaff-a871ff0bdfac",
   "metadata": {},
   "source": [
    "### In this section of the code, we first prepare our dataset for analysis by calling the `prepare_data` function. After preparing the data, we proceed to forecast using the `forecast` function, which applies predictive modeling to generate forecasts based on the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15ae32e9-a1aa-439e-b1a2-1735eaba21ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pase 1\n",
      "[] \n",
      "###########################################\n",
      "pase 2\n",
      "   Year Hand capacity_Engine Price     Km Authorized_service KM_Per_Year  \\\n",
      "0  2015    2        1600.000  None  80000                  0    8888.889   \n",
      "\n",
      "  Engine_Efficiency Vintage_Car  \n",
      "0             0.180           0   \n",
      "###########################################\n",
      "pase 3\n",
      "   Year Hand capacity_Engine Price     Km Authorized_service KM_Per_Year  \\\n",
      "0  2015    2        1600.000  None  80000                  0    8888.889   \n",
      "\n",
      "  Engine_Efficiency Vintage_Car  \n",
      "0             0.180           0   \n",
      "###########################################\n"
     ]
    }
   ],
   "source": [
    "df = prepare_data(df)\n",
    "\n",
    "# save the DataFrame in CSV file. \n",
    "df.to_csv('ready_for_modeling.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f6dc8-f1e9-4715-b1b8-a25bb1975d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcast(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
