{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7782d84",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7cea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\amitl\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\amitl\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\amitl\\anaconda3\\lib\\site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\amitl\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\amitl\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import urllib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918db06",
   "metadata": {},
   "source": [
    "# File reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c78133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/amitl/OneDrive/שולחן העבודה/שנה ג/למידת מכונה/dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63de0e",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47485b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \n",
    "    # After reviewing the 'manufactor' column, we found duplicates\n",
    "    df['manufactor'] = df['manufactor'].replace('Lexsus', 'לקסוס')\n",
    "\n",
    "    # After revewing the model column, we saw that some of the model names are written with the name of the manufactor and\n",
    "    # the year saw we build a function to help us clean the column:\n",
    "    def clean_model_name(manufactor, model_name):\n",
    "        model_name = re.sub(r'\\(\\d{4}\\)', '', model_name)\n",
    "        model_name = re.sub(manufactor, '', model_name, flags=re.IGNORECASE)\n",
    "        model_name = model_name.strip()\n",
    "        return model_name\n",
    "\n",
    "    df['model'] = df.apply(lambda row: clean_model_name(row['manufactor'], row['model']), axis=1)\n",
    "    \n",
    "    # Further cleaning 'model', 'Gear', and 'Engine_type' columns for remaining duplicates\n",
    "    df['model'] = df['model'].replace('CIVIC', 'סיוויק')\n",
    "    df['model'] = df['model'].replace('ACCORD', 'אקורד')\n",
    "    df['model'] = df['model'].replace(\"ג`טה\", \"ג'טה\")\n",
    "    df['model'] = df['model'].replace(\"ג'אז\", \"ג`אז\")\n",
    "    df['model'] = df['model'].replace('מיטו / MITO', 'מיטו')\n",
    "    df['model'] = df['model'].replace(\"ג'וק JUKE\", \"ג'וק\")\n",
    "    df['Gear'] = df['Gear'].replace('לא מוגדר', np.nan)\n",
    "    df['Gear'] = df['Gear'].replace('אוטומט', 'אוטומטית')\n",
    "    df['Engine_type'] = df['Engine_type'].replace('היבריד', 'היברידי')\n",
    "    \n",
    "    # Converting 'capacity_Engine' to numeric and removing outliers, assuming minimum capacity is 800\n",
    "    df['capacity_Engine'] = pd.to_numeric(df['capacity_Engine'], errors='coerce')\n",
    "    df.loc[df['capacity_Engine'] < 800, 'capacity_Engine'] = np.nan\n",
    "    \n",
    "    # We assume that cars with the same manufactor and model will have the same capacity engine, gear and engine type\n",
    "    def fill_missing_values(row, reference_data, columns_to_fill, reference_columns):\n",
    "        for col in columns_to_fill:\n",
    "            if pd.isnull(row[col]):\n",
    "                # find a not missing values from similar cars\n",
    "                similar_rows = reference_data[\n",
    "                    (reference_data[reference_columns[0]] == row[reference_columns[0]]) &\n",
    "                    (reference_data[reference_columns[1]] == row[reference_columns[1]])\n",
    "                ]\n",
    "                if not similar_rows.empty:\n",
    "                    non_na_values = similar_rows[col].dropna()\n",
    "                    if not non_na_values.empty:\n",
    "                        row[col] = non_na_values.mode().values[0]\n",
    "        return row\n",
    "    \n",
    "    columns_to_fill = ['Gear', 'Engine_type', 'capacity_Engine']\n",
    "    reference_columns = ['manufactor', 'model']\n",
    "    df = df.apply(lambda row: fill_missing_values(row, df, columns_to_fill, reference_columns), axis=1)\n",
    "    \n",
    "    # If there are still missing values, fill them with the most common value\n",
    "    df['Engine_type'] = df['Engine_type'].replace(np.nan, 'דיזל')\n",
    "    df['Gear'] = df['Gear'].replace(np.nan, 'אוטומטית')\n",
    "    \n",
    "    # Correcting engine capacity for Mitsubishi models based on additional research\n",
    "    df.loc[(df['model'] == 'לנסר') | (df['model'] == 'לנסר הדור החדש'), 'capacity_Engine'] = 1500\n",
    "    df.loc[df['model'] == \"אטראז'\", 'capacity_Engine'] = 1200\n",
    "    df.loc[df['model'] == 'גרנדיס', 'capacity_Engine'] = 2400\n",
    "    df.loc[df['model'] == 'IS300H', 'capacity_Engine'] = 2500\n",
    "    df.loc[df['model'] == 'GT3000', 'capacity_Engine'] = 1600\n",
    "    \n",
    "    # Removing rows with missing 'capacity_Engine' because most of the data is non-null\n",
    "    df = df.dropna(subset=['capacity_Engine'])\n",
    "    \n",
    "    # Assuming the previous owner of a first-hand car is the manufacturer\n",
    "    df.loc[df['Hand'] == 1, 'Prev_ownership'] = 'יצרן'\n",
    "    \n",
    "    # Merging categories in 'Prev_ownership' with small value counts\n",
    "    categories_to_merge = [\"ליסינג\", \"השכרה\", \"חברה\", \"ממשלתי\", \"אחר\", \"מונית\"]\n",
    "    df['Prev_ownership'] = df['Prev_ownership'].replace('לא מוגדר', np.nan)\n",
    "    df['Prev_ownership'] = df['Prev_ownership'].replace(categories_to_merge, 'לא פרטית')\n",
    "    \n",
    "    # We assume that the previous owner of a car which is first hand is the manufactor\n",
    "    df.loc[df['Hand'] == 1, 'Prev_ownership'] = 'יצרן'\n",
    "    \n",
    "    # Function to fill missing values in the \"Prev_ownership column\" by the distribution\n",
    "    def fill_na_proportionally(series):\n",
    "        # finding the distribution\n",
    "        distribution = series[series != 'יצרן'].value_counts(normalize=True)\n",
    "        # Creating a copy\n",
    "        series_copy = series.copy()\n",
    "        # finding missing indices\n",
    "        na_indices = series_copy[series_copy.isna()].index\n",
    "        # creating random values by the distribution\n",
    "        fill_values = np.random.choice(distribution.index, size=len(na_indices), p=distribution.values)\n",
    "        # filling missing values\n",
    "        series_copy.loc[na_indices] = fill_values\n",
    "        return series_copy\n",
    "\n",
    "    df['Prev_ownership'] = fill_na_proportionally(df['Prev_ownership'])\n",
    "    \n",
    "    # Dropping 'Curr_ownership' column because we assume that the current ownership is private\n",
    "    df = df.drop(columns=['Curr_ownership'])\n",
    "    \n",
    "    # Dropping 'City' and 'Area' columns due to messiness and assumed irrelevance to price\n",
    "    df = df.drop(columns=['City', 'Area'])\n",
    "    \n",
    "    # Converting 'Km' column from string to numeric format\n",
    "    df['Km'] = df['Km'].str.replace(',', '')\n",
    "    df['Km'] = df['Km'].replace({'0': np.nan, 'None': np.nan})\n",
    "    df['Km'] = pd.to_numeric(df['Km'], errors='coerce')\n",
    "    \n",
    "    # Correcting mileage written in thousands (140 instead of 140,000)\n",
    "    df.loc[df['Km'] < 1000, 'Km'] = df['Km'] * 1000\n",
    "    np.set_printoptions(suppress=True)\n",
    "   \n",
    "    # Filling missing 'Km' values based on average annual mileage data\n",
    "    def fill_km(row):\n",
    "        if pd.isna(row['Km']):\n",
    "            age = 2024 - row['Year']\n",
    "            if row['Prev_ownership'] in ['פרטית', 'יצרן']:\n",
    "                return age * 15300\n",
    "            elif row['Prev_ownership'] in ['לא פרטית']:\n",
    "                return age * 27400\n",
    "        return row['Km']\n",
    "    \n",
    "    df['Km'] = df.apply(fill_km, axis=1)\n",
    "    \n",
    "    # Feature Engineering:\n",
    "    \n",
    "    # Authorized Service - A binary column indicating whether the car is serviced at an authorized garage based on the description\n",
    "    df['Authorized_service'] = np.where(df['Description'].str.contains('מוסך מורשה', na=False), 1, 0)\n",
    "    \n",
    "    # Km Per Year, removing outliers\n",
    "    df[\"KM_Per_Year\"] = df[\"Km\"] / (2024 - df[\"Year\"])\n",
    "    pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "    df = df[df['KM_Per_Year'] <= 100000]\n",
    "\n",
    "    # Engine efficiency metric based on engine capacity and annual mileage\n",
    "    df['capacity_Engine'] = df['capacity_Engine'].astype(str)\n",
    "    df['capacity_Engine'] = df['capacity_Engine'].str.replace(',', '').astype(float)\n",
    "    df['Engine_Efficiency'] = df['capacity_Engine'] / df['KM_Per_Year']\n",
    "    \n",
    "    # Vintage Car - A car that is at least 30 years old\n",
    "    df['Vintage_Car'] = np.where(df['Year'] < 1995, 1, 0)\n",
    "\n",
    "    # Dropping columns deemed irrelevant to the data\n",
    "    df = df.drop(columns=['Cre_date', 'Repub_date', 'Pic_num', 'Description'])\n",
    "    \n",
    "    # Dropping columns with too many missing values\n",
    "    df = df.drop(columns=['Color', 'Test', 'Supply_score'])\n",
    "    \n",
    "    # Preparing categorical columns for OneHotEncoder\n",
    "    categorical_columns = ['manufactor', 'model', 'Gear', 'Engine_type', 'Prev_ownership']\n",
    "\n",
    "    # Creating ColumnTransformer with OneHotEncoder for categorical columns\n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Fitting the encoder and transforming categorical columns\n",
    "    df_encoded = column_transformer.fit_transform(df)\n",
    "\n",
    "    # Getting names of the encoded columns\n",
    "    encoded_columns = column_transformer.named_transformers_['cat'].get_feature_names_out(categorical_columns)\n",
    "\n",
    "    # Creating a new DataFrame with encoded columns and remaining original columns\n",
    "    df_encoded = pd.DataFrame(df_encoded, columns=encoded_columns.tolist() + df.drop(columns=categorical_columns).columns.tolist())\n",
    "\n",
    "    # Converting encoded columns to numeric type\n",
    "    for col in encoded_columns:\n",
    "        df_encoded[col] = pd.to_numeric(df_encoded[col])\n",
    "\n",
    "    df = df_encoded\n",
    "\n",
    "    # Changing all columns to their correct type\n",
    "    # Adjust the types according to the columns available after transformation\n",
    "    columns_to_convert = {\n",
    "        'Year': 'int',  \n",
    "        'Hand': 'int', \n",
    "        'capacity_Engine': 'float', \n",
    "        'Price': 'float', \n",
    "        'Km': 'float',        \n",
    "        \"Authorized_service\": 'int',\n",
    "        \"KM_Per_Year\": 'float',               \n",
    "        \"Engine_Efficiency\": 'float',\n",
    "        \"Vintage_Car\": 'int'\n",
    "    }\n",
    "\n",
    "    for col, col_type in columns_to_convert.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(col_type)\n",
    "\n",
    "    return df\n",
    "\n",
    "df=prepare_data (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5982aa7",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20829d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__alpha': 0.05994842503189409, 'model__l1_ratio': 0.0}\n",
      "RMSE: 8809.715891821052\n",
      "Top 5 features:\n",
      "                Feature  Importance    Effect\n",
      "260                Year   13899.741  Positive\n",
      "263                  Km    6434.992  Negative\n",
      "267         Vintage_Car    3759.277  Positive\n",
      "170  model_לנסר ספורטבק    2955.092  Negative\n",
      "12     manufactor_מרצדס    2519.448  Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitl\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.771e+10, tolerance: 6.929e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\amitl\\AppData\\Local\\Temp/ipykernel_16736/1141631157.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_5_features['Effect'] = top_5_features['Coefficient'].apply(lambda x: 'Positive' if x > 0 else 'Negative')\n"
     ]
    }
   ],
   "source": [
    "# Separating the target variable from the features\n",
    "y = df['Price']\n",
    "X = df.drop(columns=['Price'])\n",
    "\n",
    "# Creating a pipeline for scaling and model training\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scaling all features\n",
    "    ('model', ElasticNet())\n",
    "])\n",
    "\n",
    "# Defining the parameter grid for the search\n",
    "param_grid = {\n",
    "    'model__alpha': np.logspace(-4, 1, 10),\n",
    "    'model__l1_ratio': np.linspace(0, 1, 10)\n",
    "}\n",
    "\n",
    "# Performing Grid Search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Getting the best results\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Evaluating the model on the entire data\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Limiting the prediction value\n",
    "y_pred = np.where(y_pred < 10000, 15000, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Getting the coefficients of the model after scaling\n",
    "model = best_model.named_steps['model']\n",
    "scaler = best_model.named_steps['scaler']\n",
    "coefficients = model.coef_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Creating a DataFrame with the coefficients and their importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': np.abs(coefficients),\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "feature_importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Top five features\n",
    "top_5_features = feature_importance.head(5)\n",
    "top_5_features['Effect'] = top_5_features['Coefficient'].apply(lambda x: 'Positive' if x > 0 else 'Negative')\n",
    "\n",
    "print(\"Top 5 features:\")\n",
    "print(top_5_features[['Feature', 'Importance', 'Effect']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
